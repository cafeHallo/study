{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "afb2eb55-39b6-4569-a0c4-37d9703a8b0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import copy\n",
    "from torchao.quantization import Int8DynamicActivationInt4WeightConfig, Int8DynamicActivationInt8WeightConfig, Int8WeightOnlyConfig, quantize_\n",
    "from torchvision import datasets, models, transforms\n",
    "from tqdm import tqdm\n",
    "from torch.utils.data import DataLoader, random_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c5bc489c-f74e-4d9c-927a-5b48be9985bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- MHA を Q/K/V 分解して nn.Linear に置き換える ---\n",
    "class MHAWithExplicitLinear(nn.Module):\n",
    "    def __init__(self, mha: nn.MultiheadAttention):\n",
    "        super().__init__()\n",
    "        self.embed_dim = mha.embed_dim\n",
    "        self.num_heads = mha.num_heads\n",
    "        self.dropout = mha.dropout\n",
    "        self.batch_first = mha.batch_first\n",
    "\n",
    "        # in_proj を分割して q/k/v の Linear を作成\n",
    "        w = mha.in_proj_weight\n",
    "        b = mha.in_proj_bias\n",
    "        d = self.embed_dim\n",
    "\n",
    "        self.q_proj = nn.Linear(d, d, bias=b is not None)\n",
    "        self.k_proj = nn.Linear(d, d, bias=b is not None)\n",
    "        self.v_proj = nn.Linear(d, d, bias=b is not None)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            self.q_proj.weight.copy_(w[:d, :])\n",
    "            self.k_proj.weight.copy_(w[d:2*d, :])\n",
    "            self.v_proj.weight.copy_(w[2*d:, :])\n",
    "            if b is not None:\n",
    "                self.q_proj.bias.copy_(b[:d])\n",
    "                self.k_proj.bias.copy_(b[d:2*d])\n",
    "                self.v_proj.bias.copy_(b[2*d:])\n",
    "\n",
    "        # out_proj はそのままコピー\n",
    "        self.out_proj = copy.deepcopy(mha.out_proj)\n",
    "\n",
    "    def forward(self, query, key, value, need_weights=False, attn_mask=None):\n",
    "        q = self.q_proj(query)\n",
    "        k = self.k_proj(key)\n",
    "        v = self.v_proj(value)\n",
    "\n",
    "        attn_output, attn_output_weights = nn.functional.multi_head_attention_forward(\n",
    "            query=q,\n",
    "            key=k,\n",
    "            value=v,\n",
    "            embed_dim_to_check=self.embed_dim,\n",
    "            num_heads=self.num_heads,\n",
    "            in_proj_weight=None,\n",
    "            in_proj_bias=None,\n",
    "            bias_k=None,\n",
    "            bias_v=None,\n",
    "            add_zero_attn=False,\n",
    "            dropout_p=self.dropout,\n",
    "            out_proj_weight=self.out_proj.weight,\n",
    "            out_proj_bias=self.out_proj.bias,\n",
    "            training=self.training,\n",
    "            key_padding_mask=None,\n",
    "            need_weights=need_weights,\n",
    "            attn_mask=attn_mask,\n",
    "            use_separate_proj_weight=True,\n",
    "            q_proj_weight=self.q_proj.weight,\n",
    "            k_proj_weight=self.k_proj.weight,\n",
    "            v_proj_weight=self.v_proj.weight,\n",
    "            static_k=None,\n",
    "            static_v=None\n",
    "        )\n",
    "        return attn_output, attn_output_weights\n",
    "\n",
    "\n",
    "def convert_mha_to_linear_proj(model):\n",
    "    \"\"\"\n",
    "    モデル内の nn.MultiheadAttention を MHAWithExplicitLinear に置換\n",
    "    \"\"\"\n",
    "    targets = []\n",
    "    for name, module in model.named_modules():\n",
    "        if isinstance(module, nn.MultiheadAttention):\n",
    "            targets.append((name, module))\n",
    "\n",
    "    for full_name, old_mod in targets:\n",
    "        new_mod = MHAWithExplicitLinear(old_mod)\n",
    "        # 親モジュールをたどって置換\n",
    "        name_parts = full_name.split(\".\")\n",
    "        parent = model\n",
    "        for p in name_parts[:-1]:\n",
    "            parent = getattr(parent, p)\n",
    "        setattr(parent, name_parts[-1], new_mod)\n",
    "        print(f\"Replaced {full_name} -> MHAWithExplicitLinear\")\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "# --- 量子化関数 ---\n",
    "def apply_mixed_quantization(model, qcfg_qkv, qcfg_out, qcfg_other):\n",
    "    \"\"\"\n",
    "    Q/K/V, out_proj, その他 Linear をそれぞれ異なる qconfig で量子化\n",
    "    \"\"\"\n",
    "    for name, module in model.named_modules():\n",
    "        if isinstance(module, nn.Linear):\n",
    "            if any(x in name for x in [\"q_proj\", \"k_proj\", \"v_proj\"]):\n",
    "                quantize_(module, qcfg_qkv)\n",
    "                print(f\"{name} -> Q/K/V qconfig\")\n",
    "            elif \"out_proj\" in name:\n",
    "                quantize_(module, qcfg_out)\n",
    "                print(f\"{name} -> out_proj qconfig\")\n",
    "            else:\n",
    "                quantize_(module, qcfg_other)\n",
    "                print(f\"{name} -> other Linear qconfig\")\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "770f1559-611d-48e1-812c-2513a10e5eef",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Replaced encoder.layers.encoder_layer_0.self_attention -> MHAWithExplicitLinear\n",
      "Replaced encoder.layers.encoder_layer_1.self_attention -> MHAWithExplicitLinear\n",
      "Replaced encoder.layers.encoder_layer_2.self_attention -> MHAWithExplicitLinear\n",
      "Replaced encoder.layers.encoder_layer_3.self_attention -> MHAWithExplicitLinear\n",
      "Replaced encoder.layers.encoder_layer_4.self_attention -> MHAWithExplicitLinear\n",
      "Replaced encoder.layers.encoder_layer_5.self_attention -> MHAWithExplicitLinear\n",
      "Replaced encoder.layers.encoder_layer_6.self_attention -> MHAWithExplicitLinear\n",
      "Replaced encoder.layers.encoder_layer_7.self_attention -> MHAWithExplicitLinear\n",
      "Replaced encoder.layers.encoder_layer_8.self_attention -> MHAWithExplicitLinear\n",
      "Replaced encoder.layers.encoder_layer_9.self_attention -> MHAWithExplicitLinear\n",
      "Replaced encoder.layers.encoder_layer_10.self_attention -> MHAWithExplicitLinear\n",
      "Replaced encoder.layers.encoder_layer_11.self_attention -> MHAWithExplicitLinear\n",
      "encoder.layers.encoder_layer_0.self_attention.q_proj -> Q/K/V qconfig\n",
      "encoder.layers.encoder_layer_0.self_attention.k_proj -> Q/K/V qconfig\n",
      "encoder.layers.encoder_layer_0.self_attention.v_proj -> Q/K/V qconfig\n",
      "encoder.layers.encoder_layer_0.self_attention.out_proj -> out_proj qconfig\n",
      "encoder.layers.encoder_layer_0.mlp.0 -> other Linear qconfig\n",
      "encoder.layers.encoder_layer_0.mlp.3 -> other Linear qconfig\n",
      "encoder.layers.encoder_layer_1.self_attention.q_proj -> Q/K/V qconfig\n",
      "encoder.layers.encoder_layer_1.self_attention.k_proj -> Q/K/V qconfig\n",
      "encoder.layers.encoder_layer_1.self_attention.v_proj -> Q/K/V qconfig\n",
      "encoder.layers.encoder_layer_1.self_attention.out_proj -> out_proj qconfig\n",
      "encoder.layers.encoder_layer_1.mlp.0 -> other Linear qconfig\n",
      "encoder.layers.encoder_layer_1.mlp.3 -> other Linear qconfig\n",
      "encoder.layers.encoder_layer_2.self_attention.q_proj -> Q/K/V qconfig\n",
      "encoder.layers.encoder_layer_2.self_attention.k_proj -> Q/K/V qconfig\n",
      "encoder.layers.encoder_layer_2.self_attention.v_proj -> Q/K/V qconfig\n",
      "encoder.layers.encoder_layer_2.self_attention.out_proj -> out_proj qconfig\n",
      "encoder.layers.encoder_layer_2.mlp.0 -> other Linear qconfig\n",
      "encoder.layers.encoder_layer_2.mlp.3 -> other Linear qconfig\n",
      "encoder.layers.encoder_layer_3.self_attention.q_proj -> Q/K/V qconfig\n",
      "encoder.layers.encoder_layer_3.self_attention.k_proj -> Q/K/V qconfig\n",
      "encoder.layers.encoder_layer_3.self_attention.v_proj -> Q/K/V qconfig\n",
      "encoder.layers.encoder_layer_3.self_attention.out_proj -> out_proj qconfig\n",
      "encoder.layers.encoder_layer_3.mlp.0 -> other Linear qconfig\n",
      "encoder.layers.encoder_layer_3.mlp.3 -> other Linear qconfig\n",
      "encoder.layers.encoder_layer_4.self_attention.q_proj -> Q/K/V qconfig\n",
      "encoder.layers.encoder_layer_4.self_attention.k_proj -> Q/K/V qconfig\n",
      "encoder.layers.encoder_layer_4.self_attention.v_proj -> Q/K/V qconfig\n",
      "encoder.layers.encoder_layer_4.self_attention.out_proj -> out_proj qconfig\n",
      "encoder.layers.encoder_layer_4.mlp.0 -> other Linear qconfig\n",
      "encoder.layers.encoder_layer_4.mlp.3 -> other Linear qconfig\n",
      "encoder.layers.encoder_layer_5.self_attention.q_proj -> Q/K/V qconfig\n",
      "encoder.layers.encoder_layer_5.self_attention.k_proj -> Q/K/V qconfig\n",
      "encoder.layers.encoder_layer_5.self_attention.v_proj -> Q/K/V qconfig\n",
      "encoder.layers.encoder_layer_5.self_attention.out_proj -> out_proj qconfig\n",
      "encoder.layers.encoder_layer_5.mlp.0 -> other Linear qconfig\n",
      "encoder.layers.encoder_layer_5.mlp.3 -> other Linear qconfig\n",
      "encoder.layers.encoder_layer_6.self_attention.q_proj -> Q/K/V qconfig\n",
      "encoder.layers.encoder_layer_6.self_attention.k_proj -> Q/K/V qconfig\n",
      "encoder.layers.encoder_layer_6.self_attention.v_proj -> Q/K/V qconfig\n",
      "encoder.layers.encoder_layer_6.self_attention.out_proj -> out_proj qconfig\n",
      "encoder.layers.encoder_layer_6.mlp.0 -> other Linear qconfig\n",
      "encoder.layers.encoder_layer_6.mlp.3 -> other Linear qconfig\n",
      "encoder.layers.encoder_layer_7.self_attention.q_proj -> Q/K/V qconfig\n",
      "encoder.layers.encoder_layer_7.self_attention.k_proj -> Q/K/V qconfig\n",
      "encoder.layers.encoder_layer_7.self_attention.v_proj -> Q/K/V qconfig\n",
      "encoder.layers.encoder_layer_7.self_attention.out_proj -> out_proj qconfig\n",
      "encoder.layers.encoder_layer_7.mlp.0 -> other Linear qconfig\n",
      "encoder.layers.encoder_layer_7.mlp.3 -> other Linear qconfig\n",
      "encoder.layers.encoder_layer_8.self_attention.q_proj -> Q/K/V qconfig\n",
      "encoder.layers.encoder_layer_8.self_attention.k_proj -> Q/K/V qconfig\n",
      "encoder.layers.encoder_layer_8.self_attention.v_proj -> Q/K/V qconfig\n",
      "encoder.layers.encoder_layer_8.self_attention.out_proj -> out_proj qconfig\n",
      "encoder.layers.encoder_layer_8.mlp.0 -> other Linear qconfig\n",
      "encoder.layers.encoder_layer_8.mlp.3 -> other Linear qconfig\n",
      "encoder.layers.encoder_layer_9.self_attention.q_proj -> Q/K/V qconfig\n",
      "encoder.layers.encoder_layer_9.self_attention.k_proj -> Q/K/V qconfig\n",
      "encoder.layers.encoder_layer_9.self_attention.v_proj -> Q/K/V qconfig\n",
      "encoder.layers.encoder_layer_9.self_attention.out_proj -> out_proj qconfig\n",
      "encoder.layers.encoder_layer_9.mlp.0 -> other Linear qconfig\n",
      "encoder.layers.encoder_layer_9.mlp.3 -> other Linear qconfig\n",
      "encoder.layers.encoder_layer_10.self_attention.q_proj -> Q/K/V qconfig\n",
      "encoder.layers.encoder_layer_10.self_attention.k_proj -> Q/K/V qconfig\n",
      "encoder.layers.encoder_layer_10.self_attention.v_proj -> Q/K/V qconfig\n",
      "encoder.layers.encoder_layer_10.self_attention.out_proj -> out_proj qconfig\n",
      "encoder.layers.encoder_layer_10.mlp.0 -> other Linear qconfig\n",
      "encoder.layers.encoder_layer_10.mlp.3 -> other Linear qconfig\n",
      "encoder.layers.encoder_layer_11.self_attention.q_proj -> Q/K/V qconfig\n",
      "encoder.layers.encoder_layer_11.self_attention.k_proj -> Q/K/V qconfig\n",
      "encoder.layers.encoder_layer_11.self_attention.v_proj -> Q/K/V qconfig\n",
      "encoder.layers.encoder_layer_11.self_attention.out_proj -> out_proj qconfig\n",
      "encoder.layers.encoder_layer_11.mlp.0 -> other Linear qconfig\n",
      "encoder.layers.encoder_layer_11.mlp.3 -> other Linear qconfig\n",
      "heads.head -> other Linear qconfig\n"
     ]
    }
   ],
   "source": [
    "# モデル読み込み\n",
    "model = torch.load(\"model_full.pth\", weights_only=False)\n",
    "model.eval()\n",
    "\n",
    "model_fp32 = copy.deepcopy(model).to(torch.float32)\n",
    "\n",
    "# MHA を Q/K/V/Out の Linear に展開\n",
    "model_fp32 = convert_mha_to_linear_proj(model_fp32)\n",
    "\n",
    "# 好きな qconfig を指定可能\n",
    "qcfg_qkv   = Int8WeightOnlyConfig()   # 例: Q/K/V は Int8\n",
    "qcfg_out   = Int8DynamicActivationInt8WeightConfig()   # 例: out_proj も Int8\n",
    "qcfg_other = Int8DynamicActivationInt8WeightConfig()   # 例: その他 Linear は Int4\n",
    "\n",
    "model_fp32 = apply_mixed_quantization(model_fp32, qcfg_qkv, qcfg_out, qcfg_other)\n",
    "model_fp32.eval()\n",
    "\n",
    "torch.save(model_fp32, \"model_custom_quant.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d4e4630c-d14e-4671-811f-61c8c6747ee2",
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = models.ViT_B_16_Weights.DEFAULT\n",
    "transform = weights.transforms()\n",
    "\n",
    "train_dataset = datasets.ImageFolder(\"dataset/train\", transform=transform)\n",
    "val_dataset   = datasets.ImageFolder(\"dataset/val\", transform=transform)\n",
    "test_dataset  = datasets.ImageFolder(\"dataset/test\", transform=transform)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "val_loader   = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
    "test_loader  = DataLoader(test_dataset, batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7bd02a7e-735f-4e96-a996-85ac21eaf724",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VisionTransformer(\n",
       "  (conv_proj): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))\n",
       "  (encoder): Encoder(\n",
       "    (dropout): Dropout(p=0.0, inplace=False)\n",
       "    (layers): Sequential(\n",
       "      (encoder_layer_0): EncoderBlock(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (self_attention): MHAWithExplicitLinear(\n",
       "          (q_proj): Linear(in_features=768, out_features=768, weight=AffineQuantizedTensor(shape=torch.Size([768, 768]), block_size=(1, 768), device=cpu, _layout=PlainLayout(), tensor_impl_dtype=torch.int8, quant_min=None, quant_max=None))\n",
       "          (k_proj): Linear(in_features=768, out_features=768, weight=AffineQuantizedTensor(shape=torch.Size([768, 768]), block_size=(1, 768), device=cpu, _layout=PlainLayout(), tensor_impl_dtype=torch.int8, quant_min=None, quant_max=None))\n",
       "          (v_proj): Linear(in_features=768, out_features=768, weight=AffineQuantizedTensor(shape=torch.Size([768, 768]), block_size=(1, 768), device=cpu, _layout=PlainLayout(), tensor_impl_dtype=torch.int8, quant_min=None, quant_max=None))\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "        (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): MLPBlock(\n",
       "          (0): Linear(in_features=768, out_features=3072, weight=LinearActivationQuantizedTensor(activation=<function _int8_symm_per_token_reduced_range_quant at 0x7b5b5420e050>, weight=AffineQuantizedTensor(shape=torch.Size([3072, 768]), block_size=(1, 768), device=cpu, _layout=PlainLayout(), tensor_impl_dtype=torch.int8, quant_min=None, quant_max=None)))\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Dropout(p=0.0, inplace=False)\n",
       "          (3): Linear(in_features=3072, out_features=768, weight=LinearActivationQuantizedTensor(activation=<function _int8_symm_per_token_reduced_range_quant at 0x7b5b5420e050>, weight=AffineQuantizedTensor(shape=torch.Size([768, 3072]), block_size=(1, 3072), device=cpu, _layout=PlainLayout(), tensor_impl_dtype=torch.int8, quant_min=None, quant_max=None)))\n",
       "          (4): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (encoder_layer_1): EncoderBlock(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (self_attention): MHAWithExplicitLinear(\n",
       "          (q_proj): Linear(in_features=768, out_features=768, weight=AffineQuantizedTensor(shape=torch.Size([768, 768]), block_size=(1, 768), device=cpu, _layout=PlainLayout(), tensor_impl_dtype=torch.int8, quant_min=None, quant_max=None))\n",
       "          (k_proj): Linear(in_features=768, out_features=768, weight=AffineQuantizedTensor(shape=torch.Size([768, 768]), block_size=(1, 768), device=cpu, _layout=PlainLayout(), tensor_impl_dtype=torch.int8, quant_min=None, quant_max=None))\n",
       "          (v_proj): Linear(in_features=768, out_features=768, weight=AffineQuantizedTensor(shape=torch.Size([768, 768]), block_size=(1, 768), device=cpu, _layout=PlainLayout(), tensor_impl_dtype=torch.int8, quant_min=None, quant_max=None))\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "        (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): MLPBlock(\n",
       "          (0): Linear(in_features=768, out_features=3072, weight=LinearActivationQuantizedTensor(activation=<function _int8_symm_per_token_reduced_range_quant at 0x7b5b5420e050>, weight=AffineQuantizedTensor(shape=torch.Size([3072, 768]), block_size=(1, 768), device=cpu, _layout=PlainLayout(), tensor_impl_dtype=torch.int8, quant_min=None, quant_max=None)))\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Dropout(p=0.0, inplace=False)\n",
       "          (3): Linear(in_features=3072, out_features=768, weight=LinearActivationQuantizedTensor(activation=<function _int8_symm_per_token_reduced_range_quant at 0x7b5b5420e050>, weight=AffineQuantizedTensor(shape=torch.Size([768, 3072]), block_size=(1, 3072), device=cpu, _layout=PlainLayout(), tensor_impl_dtype=torch.int8, quant_min=None, quant_max=None)))\n",
       "          (4): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (encoder_layer_2): EncoderBlock(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (self_attention): MHAWithExplicitLinear(\n",
       "          (q_proj): Linear(in_features=768, out_features=768, weight=AffineQuantizedTensor(shape=torch.Size([768, 768]), block_size=(1, 768), device=cpu, _layout=PlainLayout(), tensor_impl_dtype=torch.int8, quant_min=None, quant_max=None))\n",
       "          (k_proj): Linear(in_features=768, out_features=768, weight=AffineQuantizedTensor(shape=torch.Size([768, 768]), block_size=(1, 768), device=cpu, _layout=PlainLayout(), tensor_impl_dtype=torch.int8, quant_min=None, quant_max=None))\n",
       "          (v_proj): Linear(in_features=768, out_features=768, weight=AffineQuantizedTensor(shape=torch.Size([768, 768]), block_size=(1, 768), device=cpu, _layout=PlainLayout(), tensor_impl_dtype=torch.int8, quant_min=None, quant_max=None))\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "        (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): MLPBlock(\n",
       "          (0): Linear(in_features=768, out_features=3072, weight=LinearActivationQuantizedTensor(activation=<function _int8_symm_per_token_reduced_range_quant at 0x7b5b5420e050>, weight=AffineQuantizedTensor(shape=torch.Size([3072, 768]), block_size=(1, 768), device=cpu, _layout=PlainLayout(), tensor_impl_dtype=torch.int8, quant_min=None, quant_max=None)))\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Dropout(p=0.0, inplace=False)\n",
       "          (3): Linear(in_features=3072, out_features=768, weight=LinearActivationQuantizedTensor(activation=<function _int8_symm_per_token_reduced_range_quant at 0x7b5b5420e050>, weight=AffineQuantizedTensor(shape=torch.Size([768, 3072]), block_size=(1, 3072), device=cpu, _layout=PlainLayout(), tensor_impl_dtype=torch.int8, quant_min=None, quant_max=None)))\n",
       "          (4): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (encoder_layer_3): EncoderBlock(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (self_attention): MHAWithExplicitLinear(\n",
       "          (q_proj): Linear(in_features=768, out_features=768, weight=AffineQuantizedTensor(shape=torch.Size([768, 768]), block_size=(1, 768), device=cpu, _layout=PlainLayout(), tensor_impl_dtype=torch.int8, quant_min=None, quant_max=None))\n",
       "          (k_proj): Linear(in_features=768, out_features=768, weight=AffineQuantizedTensor(shape=torch.Size([768, 768]), block_size=(1, 768), device=cpu, _layout=PlainLayout(), tensor_impl_dtype=torch.int8, quant_min=None, quant_max=None))\n",
       "          (v_proj): Linear(in_features=768, out_features=768, weight=AffineQuantizedTensor(shape=torch.Size([768, 768]), block_size=(1, 768), device=cpu, _layout=PlainLayout(), tensor_impl_dtype=torch.int8, quant_min=None, quant_max=None))\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "        (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): MLPBlock(\n",
       "          (0): Linear(in_features=768, out_features=3072, weight=LinearActivationQuantizedTensor(activation=<function _int8_symm_per_token_reduced_range_quant at 0x7b5b5420e050>, weight=AffineQuantizedTensor(shape=torch.Size([3072, 768]), block_size=(1, 768), device=cpu, _layout=PlainLayout(), tensor_impl_dtype=torch.int8, quant_min=None, quant_max=None)))\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Dropout(p=0.0, inplace=False)\n",
       "          (3): Linear(in_features=3072, out_features=768, weight=LinearActivationQuantizedTensor(activation=<function _int8_symm_per_token_reduced_range_quant at 0x7b5b5420e050>, weight=AffineQuantizedTensor(shape=torch.Size([768, 3072]), block_size=(1, 3072), device=cpu, _layout=PlainLayout(), tensor_impl_dtype=torch.int8, quant_min=None, quant_max=None)))\n",
       "          (4): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (encoder_layer_4): EncoderBlock(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (self_attention): MHAWithExplicitLinear(\n",
       "          (q_proj): Linear(in_features=768, out_features=768, weight=AffineQuantizedTensor(shape=torch.Size([768, 768]), block_size=(1, 768), device=cpu, _layout=PlainLayout(), tensor_impl_dtype=torch.int8, quant_min=None, quant_max=None))\n",
       "          (k_proj): Linear(in_features=768, out_features=768, weight=AffineQuantizedTensor(shape=torch.Size([768, 768]), block_size=(1, 768), device=cpu, _layout=PlainLayout(), tensor_impl_dtype=torch.int8, quant_min=None, quant_max=None))\n",
       "          (v_proj): Linear(in_features=768, out_features=768, weight=AffineQuantizedTensor(shape=torch.Size([768, 768]), block_size=(1, 768), device=cpu, _layout=PlainLayout(), tensor_impl_dtype=torch.int8, quant_min=None, quant_max=None))\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "        (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): MLPBlock(\n",
       "          (0): Linear(in_features=768, out_features=3072, weight=LinearActivationQuantizedTensor(activation=<function _int8_symm_per_token_reduced_range_quant at 0x7b5b5420e050>, weight=AffineQuantizedTensor(shape=torch.Size([3072, 768]), block_size=(1, 768), device=cpu, _layout=PlainLayout(), tensor_impl_dtype=torch.int8, quant_min=None, quant_max=None)))\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Dropout(p=0.0, inplace=False)\n",
       "          (3): Linear(in_features=3072, out_features=768, weight=LinearActivationQuantizedTensor(activation=<function _int8_symm_per_token_reduced_range_quant at 0x7b5b5420e050>, weight=AffineQuantizedTensor(shape=torch.Size([768, 3072]), block_size=(1, 3072), device=cpu, _layout=PlainLayout(), tensor_impl_dtype=torch.int8, quant_min=None, quant_max=None)))\n",
       "          (4): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (encoder_layer_5): EncoderBlock(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (self_attention): MHAWithExplicitLinear(\n",
       "          (q_proj): Linear(in_features=768, out_features=768, weight=AffineQuantizedTensor(shape=torch.Size([768, 768]), block_size=(1, 768), device=cpu, _layout=PlainLayout(), tensor_impl_dtype=torch.int8, quant_min=None, quant_max=None))\n",
       "          (k_proj): Linear(in_features=768, out_features=768, weight=AffineQuantizedTensor(shape=torch.Size([768, 768]), block_size=(1, 768), device=cpu, _layout=PlainLayout(), tensor_impl_dtype=torch.int8, quant_min=None, quant_max=None))\n",
       "          (v_proj): Linear(in_features=768, out_features=768, weight=AffineQuantizedTensor(shape=torch.Size([768, 768]), block_size=(1, 768), device=cpu, _layout=PlainLayout(), tensor_impl_dtype=torch.int8, quant_min=None, quant_max=None))\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "        (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): MLPBlock(\n",
       "          (0): Linear(in_features=768, out_features=3072, weight=LinearActivationQuantizedTensor(activation=<function _int8_symm_per_token_reduced_range_quant at 0x7b5b5420e050>, weight=AffineQuantizedTensor(shape=torch.Size([3072, 768]), block_size=(1, 768), device=cpu, _layout=PlainLayout(), tensor_impl_dtype=torch.int8, quant_min=None, quant_max=None)))\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Dropout(p=0.0, inplace=False)\n",
       "          (3): Linear(in_features=3072, out_features=768, weight=LinearActivationQuantizedTensor(activation=<function _int8_symm_per_token_reduced_range_quant at 0x7b5b5420e050>, weight=AffineQuantizedTensor(shape=torch.Size([768, 3072]), block_size=(1, 3072), device=cpu, _layout=PlainLayout(), tensor_impl_dtype=torch.int8, quant_min=None, quant_max=None)))\n",
       "          (4): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (encoder_layer_6): EncoderBlock(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (self_attention): MHAWithExplicitLinear(\n",
       "          (q_proj): Linear(in_features=768, out_features=768, weight=AffineQuantizedTensor(shape=torch.Size([768, 768]), block_size=(1, 768), device=cpu, _layout=PlainLayout(), tensor_impl_dtype=torch.int8, quant_min=None, quant_max=None))\n",
       "          (k_proj): Linear(in_features=768, out_features=768, weight=AffineQuantizedTensor(shape=torch.Size([768, 768]), block_size=(1, 768), device=cpu, _layout=PlainLayout(), tensor_impl_dtype=torch.int8, quant_min=None, quant_max=None))\n",
       "          (v_proj): Linear(in_features=768, out_features=768, weight=AffineQuantizedTensor(shape=torch.Size([768, 768]), block_size=(1, 768), device=cpu, _layout=PlainLayout(), tensor_impl_dtype=torch.int8, quant_min=None, quant_max=None))\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "        (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): MLPBlock(\n",
       "          (0): Linear(in_features=768, out_features=3072, weight=LinearActivationQuantizedTensor(activation=<function _int8_symm_per_token_reduced_range_quant at 0x7b5b5420e050>, weight=AffineQuantizedTensor(shape=torch.Size([3072, 768]), block_size=(1, 768), device=cpu, _layout=PlainLayout(), tensor_impl_dtype=torch.int8, quant_min=None, quant_max=None)))\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Dropout(p=0.0, inplace=False)\n",
       "          (3): Linear(in_features=3072, out_features=768, weight=LinearActivationQuantizedTensor(activation=<function _int8_symm_per_token_reduced_range_quant at 0x7b5b5420e050>, weight=AffineQuantizedTensor(shape=torch.Size([768, 3072]), block_size=(1, 3072), device=cpu, _layout=PlainLayout(), tensor_impl_dtype=torch.int8, quant_min=None, quant_max=None)))\n",
       "          (4): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (encoder_layer_7): EncoderBlock(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (self_attention): MHAWithExplicitLinear(\n",
       "          (q_proj): Linear(in_features=768, out_features=768, weight=AffineQuantizedTensor(shape=torch.Size([768, 768]), block_size=(1, 768), device=cpu, _layout=PlainLayout(), tensor_impl_dtype=torch.int8, quant_min=None, quant_max=None))\n",
       "          (k_proj): Linear(in_features=768, out_features=768, weight=AffineQuantizedTensor(shape=torch.Size([768, 768]), block_size=(1, 768), device=cpu, _layout=PlainLayout(), tensor_impl_dtype=torch.int8, quant_min=None, quant_max=None))\n",
       "          (v_proj): Linear(in_features=768, out_features=768, weight=AffineQuantizedTensor(shape=torch.Size([768, 768]), block_size=(1, 768), device=cpu, _layout=PlainLayout(), tensor_impl_dtype=torch.int8, quant_min=None, quant_max=None))\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "        (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): MLPBlock(\n",
       "          (0): Linear(in_features=768, out_features=3072, weight=LinearActivationQuantizedTensor(activation=<function _int8_symm_per_token_reduced_range_quant at 0x7b5b5420e050>, weight=AffineQuantizedTensor(shape=torch.Size([3072, 768]), block_size=(1, 768), device=cpu, _layout=PlainLayout(), tensor_impl_dtype=torch.int8, quant_min=None, quant_max=None)))\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Dropout(p=0.0, inplace=False)\n",
       "          (3): Linear(in_features=3072, out_features=768, weight=LinearActivationQuantizedTensor(activation=<function _int8_symm_per_token_reduced_range_quant at 0x7b5b5420e050>, weight=AffineQuantizedTensor(shape=torch.Size([768, 3072]), block_size=(1, 3072), device=cpu, _layout=PlainLayout(), tensor_impl_dtype=torch.int8, quant_min=None, quant_max=None)))\n",
       "          (4): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (encoder_layer_8): EncoderBlock(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (self_attention): MHAWithExplicitLinear(\n",
       "          (q_proj): Linear(in_features=768, out_features=768, weight=AffineQuantizedTensor(shape=torch.Size([768, 768]), block_size=(1, 768), device=cpu, _layout=PlainLayout(), tensor_impl_dtype=torch.int8, quant_min=None, quant_max=None))\n",
       "          (k_proj): Linear(in_features=768, out_features=768, weight=AffineQuantizedTensor(shape=torch.Size([768, 768]), block_size=(1, 768), device=cpu, _layout=PlainLayout(), tensor_impl_dtype=torch.int8, quant_min=None, quant_max=None))\n",
       "          (v_proj): Linear(in_features=768, out_features=768, weight=AffineQuantizedTensor(shape=torch.Size([768, 768]), block_size=(1, 768), device=cpu, _layout=PlainLayout(), tensor_impl_dtype=torch.int8, quant_min=None, quant_max=None))\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "        (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): MLPBlock(\n",
       "          (0): Linear(in_features=768, out_features=3072, weight=LinearActivationQuantizedTensor(activation=<function _int8_symm_per_token_reduced_range_quant at 0x7b5b5420e050>, weight=AffineQuantizedTensor(shape=torch.Size([3072, 768]), block_size=(1, 768), device=cpu, _layout=PlainLayout(), tensor_impl_dtype=torch.int8, quant_min=None, quant_max=None)))\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Dropout(p=0.0, inplace=False)\n",
       "          (3): Linear(in_features=3072, out_features=768, weight=LinearActivationQuantizedTensor(activation=<function _int8_symm_per_token_reduced_range_quant at 0x7b5b5420e050>, weight=AffineQuantizedTensor(shape=torch.Size([768, 3072]), block_size=(1, 3072), device=cpu, _layout=PlainLayout(), tensor_impl_dtype=torch.int8, quant_min=None, quant_max=None)))\n",
       "          (4): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (encoder_layer_9): EncoderBlock(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (self_attention): MHAWithExplicitLinear(\n",
       "          (q_proj): Linear(in_features=768, out_features=768, weight=AffineQuantizedTensor(shape=torch.Size([768, 768]), block_size=(1, 768), device=cpu, _layout=PlainLayout(), tensor_impl_dtype=torch.int8, quant_min=None, quant_max=None))\n",
       "          (k_proj): Linear(in_features=768, out_features=768, weight=AffineQuantizedTensor(shape=torch.Size([768, 768]), block_size=(1, 768), device=cpu, _layout=PlainLayout(), tensor_impl_dtype=torch.int8, quant_min=None, quant_max=None))\n",
       "          (v_proj): Linear(in_features=768, out_features=768, weight=AffineQuantizedTensor(shape=torch.Size([768, 768]), block_size=(1, 768), device=cpu, _layout=PlainLayout(), tensor_impl_dtype=torch.int8, quant_min=None, quant_max=None))\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "        (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): MLPBlock(\n",
       "          (0): Linear(in_features=768, out_features=3072, weight=LinearActivationQuantizedTensor(activation=<function _int8_symm_per_token_reduced_range_quant at 0x7b5b5420e050>, weight=AffineQuantizedTensor(shape=torch.Size([3072, 768]), block_size=(1, 768), device=cpu, _layout=PlainLayout(), tensor_impl_dtype=torch.int8, quant_min=None, quant_max=None)))\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Dropout(p=0.0, inplace=False)\n",
       "          (3): Linear(in_features=3072, out_features=768, weight=LinearActivationQuantizedTensor(activation=<function _int8_symm_per_token_reduced_range_quant at 0x7b5b5420e050>, weight=AffineQuantizedTensor(shape=torch.Size([768, 3072]), block_size=(1, 3072), device=cpu, _layout=PlainLayout(), tensor_impl_dtype=torch.int8, quant_min=None, quant_max=None)))\n",
       "          (4): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (encoder_layer_10): EncoderBlock(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (self_attention): MHAWithExplicitLinear(\n",
       "          (q_proj): Linear(in_features=768, out_features=768, weight=AffineQuantizedTensor(shape=torch.Size([768, 768]), block_size=(1, 768), device=cpu, _layout=PlainLayout(), tensor_impl_dtype=torch.int8, quant_min=None, quant_max=None))\n",
       "          (k_proj): Linear(in_features=768, out_features=768, weight=AffineQuantizedTensor(shape=torch.Size([768, 768]), block_size=(1, 768), device=cpu, _layout=PlainLayout(), tensor_impl_dtype=torch.int8, quant_min=None, quant_max=None))\n",
       "          (v_proj): Linear(in_features=768, out_features=768, weight=AffineQuantizedTensor(shape=torch.Size([768, 768]), block_size=(1, 768), device=cpu, _layout=PlainLayout(), tensor_impl_dtype=torch.int8, quant_min=None, quant_max=None))\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "        (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): MLPBlock(\n",
       "          (0): Linear(in_features=768, out_features=3072, weight=LinearActivationQuantizedTensor(activation=<function _int8_symm_per_token_reduced_range_quant at 0x7b5b5420e050>, weight=AffineQuantizedTensor(shape=torch.Size([3072, 768]), block_size=(1, 768), device=cpu, _layout=PlainLayout(), tensor_impl_dtype=torch.int8, quant_min=None, quant_max=None)))\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Dropout(p=0.0, inplace=False)\n",
       "          (3): Linear(in_features=3072, out_features=768, weight=LinearActivationQuantizedTensor(activation=<function _int8_symm_per_token_reduced_range_quant at 0x7b5b5420e050>, weight=AffineQuantizedTensor(shape=torch.Size([768, 3072]), block_size=(1, 3072), device=cpu, _layout=PlainLayout(), tensor_impl_dtype=torch.int8, quant_min=None, quant_max=None)))\n",
       "          (4): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (encoder_layer_11): EncoderBlock(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (self_attention): MHAWithExplicitLinear(\n",
       "          (q_proj): Linear(in_features=768, out_features=768, weight=AffineQuantizedTensor(shape=torch.Size([768, 768]), block_size=(1, 768), device=cpu, _layout=PlainLayout(), tensor_impl_dtype=torch.int8, quant_min=None, quant_max=None))\n",
       "          (k_proj): Linear(in_features=768, out_features=768, weight=AffineQuantizedTensor(shape=torch.Size([768, 768]), block_size=(1, 768), device=cpu, _layout=PlainLayout(), tensor_impl_dtype=torch.int8, quant_min=None, quant_max=None))\n",
       "          (v_proj): Linear(in_features=768, out_features=768, weight=AffineQuantizedTensor(shape=torch.Size([768, 768]), block_size=(1, 768), device=cpu, _layout=PlainLayout(), tensor_impl_dtype=torch.int8, quant_min=None, quant_max=None))\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "        (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): MLPBlock(\n",
       "          (0): Linear(in_features=768, out_features=3072, weight=LinearActivationQuantizedTensor(activation=<function _int8_symm_per_token_reduced_range_quant at 0x7b5b5420e050>, weight=AffineQuantizedTensor(shape=torch.Size([3072, 768]), block_size=(1, 768), device=cpu, _layout=PlainLayout(), tensor_impl_dtype=torch.int8, quant_min=None, quant_max=None)))\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Dropout(p=0.0, inplace=False)\n",
       "          (3): Linear(in_features=3072, out_features=768, weight=LinearActivationQuantizedTensor(activation=<function _int8_symm_per_token_reduced_range_quant at 0x7b5b5420e050>, weight=AffineQuantizedTensor(shape=torch.Size([768, 3072]), block_size=(1, 3072), device=cpu, _layout=PlainLayout(), tensor_impl_dtype=torch.int8, quant_min=None, quant_max=None)))\n",
       "          (4): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "  )\n",
       "  (heads): Sequential(\n",
       "    (head): Linear(in_features=768, out_features=5, weight=LinearActivationQuantizedTensor(activation=<function _int8_symm_per_token_reduced_range_quant at 0x7b5b5420e050>, weight=AffineQuantizedTensor(shape=torch.Size([5, 768]), block_size=(1, 768), device=cpu, _layout=PlainLayout(), tensor_impl_dtype=torch.int8, quant_min=None, quant_max=None)))\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device =torch.device(\"cpu\")\n",
    "model_fp32.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a66e8922-2591-40df-a5be-dc795ab683be",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing quantized model: 100%|██████████| 14/14 [00:20<00:00,  1.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INT4 weight-only Test Accuracy: 0.2437\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# --- テスト評価（あなたの test_loader をそのまま使ってください）---\n",
    "correct, total = 0, 0\n",
    "with torch.no_grad():\n",
    "    for images, labels in tqdm(test_loader, desc=\"Testing quantized model\"):\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        outputs = model_fp32(images)\n",
    "        _, predicted = outputs.max(1)\n",
    "        total += labels.size(0)\n",
    "        correct += predicted.eq(labels).sum().item()\n",
    "test_acc = correct / total\n",
    "print(f\"INT4 weight-only Test Accuracy: {test_acc:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".vit",
   "language": "python",
   "name": ".vit"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
